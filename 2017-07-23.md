# Module 2
## Exercise 1

Searched suebi (I think this is the latin name for the Swabians)

Got several hits in Germany (as hoped)

http://edh-www.adw.uni-heidelberg.de/edh/inschrift/HD001278

Searched and found an inscription in Uzbekistan

Detailed data can be downloaded as json, link to sample below:

https://github.com/jeffblackadar/module2e4/blob/master/HD001278.json

### Searching of Commonwealth War Graves Commission

http://www.cwgc.org/

I searched by name and place.

I searched by cemetary, looking up Camp Hill cemetery in Halifax, NS

My Great Grandfather Samuel Hood served with the No.2 Construction Battalion in World War I - Known as the Black Battalion.

Here is an excerpt from the Canadian Encyclopedia

http://www.thecanadianencyclopedia.ca/en/article/no-2-construction-battalion/

    Many veterans of the Black Battalion were buried in Camp Hill Cemetery in Halifax. Each grave was marked by a flat, white stone,      forcing visitors to crouch down and grope the grass to find loved ones. In 1997–98, Senator Ruck successfully lobbied the Department    of Veterans Affairs, and each soldier received a proper headstone and inscription in 1999.

The spreadsheet I downloaded from the cwgc did not have names from No.2 Construction and that may be because members of No.2 Construction who died in World War I were not buried at Camp Hill, while veterans were.

I searched for the unit in the search engine and could not find reference to it.  Is it possible that no soldiers from their unit died in World War I?

The War Diary of Number 2 Construction, mentions 931410 pte. Some, C. was murdered.

http://data2.archives.ca/e/e060/e001476837.jpg

Using his service number, here is his record:

http://data2.archives.ca/e/e060/e001476837.jpg

Interestingly, his record can be looked up using "Canadian Railway Troops

To look up the Construction Battallion I had to type 2nd Construction Btn. In the Unit field.




## Exercise 2
Learning to use wget.

I thought about using my home machine to hold the files for the Shawville Equity so that I don't clog DHBox and also so that I don't have to move things in the middle of the project.  For now I will use DHBox so that I do what the rest of the class is doing.

### wget

Example:

wget -r --no-parent -w 5 --limit-rate=20k http://activehistory.ca/papers/

I tried --random-wait - I got an error.

:~/Spoon-Knife/wget-practice$ wget -r --no-parent -w 5 --random-wait --limit-rate=20k http://activehistory.ca/papers/

idn_encode failed (1): ‘String preparation failed’

idn_encode failed (1): ‘String preparation failed’

--2017-07-18 01:44:15--  http://%C3%A2%C2%80%C2%93%C3%A2--random-wait/

Resolving â\302\200\302\223â--random-wait (â\302\200\302\223â--random-wait)... failed: Name or service not known.

wget: unable to resolve host address ‘â\302\200\302\223â--random-wait’

#### Wget of the Shawville Equity

Started with 1 year

wget -r --no-parent -w 5 --limit-rate=20k http://collections.banq.qc.ca:8008/jrn03/equity/src/1883/

** fail ** I saw I was downloading pdfs.  Not needed.

I should be using a command like this

wget http://collections.banq.qc.ca:8008/jrn03/equity/src/1883/ -A **.txt** -r --no-parent -nd –w 2 --limit-rate=20k

### Examining downloaded files.

#### I encountered directory collections.banq.qc.ca:8008

it's too long to type all the time, tried to rename it

:~/final-project/equity/src$ rename collections.banq.qc.ca:8008 collections

syntax error at (eval 1) line 1, near "ca:"

:~/final-project/equity/src$ rename "collections.banq.qc.ca:8008" collections

syntax error at (eval 1) line 1, near "ca:"

This failed, but I was able to rename it in the file 

#### It appears I downloaded everything except .txt

:~/final-project/equity/src$ ls

collections

:~/final-project/equity/src$ cd collections

:~/final-project/equity/src/collections$ dir

jrn03

:~/final-project/equity/src/collections$ cd jrn03

:~/final-project/equity/src/collections/jrn03$ dir

equity

:~/final-project/equity/src/collections/jrn03$ cd equity

:~/final-project/equity/src/collections/jrn03/equity$ dir

src

:~/final-project/equity/src/collections/jrn03/equity$ cd src

:~/final-project/equity/src/collections/jrn03/equity/src$ dir

1883

:~/final-project/equity/src/collections/jrn03/equity/src$ cd 1883

:~/final-project/equity/src/collections/jrn03/equity/src/1883$ dir

06  07  08  09  10  11  12  index.html

:~/final-project/equity/src/collections/jrn03/equity/src/1883$ cd 06

:~/final-project/equity/src/collections/jrn03/equity/src/1883/06$ dir

07  14  21  28  index.html

:~/final-project/equity/src/collections/jrn03/equity/src/1883/06$ cd 07

:~/final-project/equity/src/collections/jrn03/equity/src/1883/06/07$ dir

83471_1883-06-07.pdf  index.html

Performed

wget http://collections.banq.qc.ca:8008/jrn03/equity/src/1883/ -A .txt -r --no-parent -nd –w 2 --limit-rate=20k

Results are now what I expect.

:~/final-project/equity$ dir

83471_1883-06-07.txt  83471_1883-07-05.txt  83471_1883-08-02.txt (....etc.)

#### Quality of OCR examples:

George Elliot's nmrsliang is Ireyond my to her children's sympathy.

Paul pro]>oiKil to <j lit up these lnmleh 1 into fragmenta,

## Close Reading with TEI

What makes you believe this site is a trustworthy provider of historical texts?

Registered charity with a number.  Is a .org web site.  Has a street address.

What makes you believe this site is NOT a trustworthy provider of historical texts?

The look of the site is somewhat dated.  There are not many names of actual people who work on the website.  It's not part of a larger institution

Opened 'blanktemplate.txt' in Netbeans (for the notepad function)

renamed file to page19.xml

Transcribed a half a page of text manually, separating each paragraph with </p> (to close the paragraph) and <p> to open a new para.

I made an .jpeg of each column of text, save that file, and uploaded it to freeocr.com.

I ran one image through freeocr.com and got some text and some unreadble characters. However, it was faster to use some of the good text than to type it.

I converted the second image to just a black and white image and this reduced some of the background smudging that stopped good character recognition.  I OCR'd that and the results, albiet for a different column of text, seemed better.

### Failures

I opened up page19.xml in Chrome, I didn't see anything per a security warning that I can't open files from my PC that reference other files (see note about .xsl below) in Chrome.  This was in the instructions, I should have read more carefully.

I opened page19.xml in Microsoft Edge and it looked like a unformed text.

I had to correct two things

In the reference tag for place names, there was an ampersand &amp; in the URL.  I had to replace the &amp; with &amp;amp;

so that ...Virginia&amp;params...  became   ...Virginia**&amp;amp;**params...

I also have to look at line 2 of the xml file that references an .xsl file (the style)  I changed the name of the .xsl to page19style.xsl and then matched that file name in the declaration of where the style sheet is in the xml file (see line below)

<?xml-stylesheet type="text/xsl" href="page19style.xsl"?>

After that it I could see style rendering and the tags working.

I will return to do more when there is time.

## Exercise 4

Performed:

http://search.canadiana.ca/search?q=%22New+Germany%22&field=&so=score&df=1800&dt=1900**&fmt=json**

Got back JSON data!  - This is wow.

         "creator" : [
            "DesBrisay, Mather B. (Mather Byles), 1828-1896"
         ],
         "note" : [
            "3 microfiches (141 fr.).",
            "\"Follow me back through a hoary century.\"",
            "Includes index.",
            "264 p."
         ],
         "title" : [
            "History of the County of Lunenburg",
            "CIHM/ICMH microfiche series ; no. 06874"
         ],


As an aside, I like how this code looks above.  Not sure how to make it look like that when I want.

#### Installed jq

         :~$ sudo apt-get install jq -y
         :~$ mkdir m2e4
         :~$ cd m2e4
         :~/m2e4$ pwd
         /home/jeffblackadar/m2e4
         :~/m2e4$ touch canadiana.sh
         :~/m2e4$ nano canadiana.sh

Further aside, if I add 10 empty spaces, I get the code formatting above.  Looks better to me.

### edit Ian Milligan's program

         #! /bin/bash
         pages=$(curl 'http://search.canadiana.ca/search?q="shawville"*&field=&so=score&df=1883&dt=1976&fmt=json' | jq '.pages')
         # this goes into the results and reads the value of 'pages' in each one of them.
         # it then tells us how many pages we're going to have.
         echo "Pages:"$pages
         # this says 'for each one of these pages, download the 'key' value on each page'
         for i in $(seq 1 $pages)
         do
                 curl 'http://search.canadiana.ca/search/'${i}'?q=montenegr*&field=&so=score&df=1883&dt=1976&fmt=json' | jq '.docs[] | {key}' >> results.txt
         done
         # the next two lines take the results.txt file that is quite messy and clean it up.
         # I'll try to explain what they all mean. 
         # Basically, tr deletes a given character - so we delete quotation marks "\"" 
         # (the slash tells the computer not to treat the quotation mark as a computer code, but as a quotation mark itself), 
         # to erase spaces, and to find the phrase "key:" and delete it too.
         sed -e 's/\"key\": \"//g' results.txt | tr -d "\"" | tr -d "{" | tr -d "}" | tr -s " " | sed '/^\s*$/d' | tr -d ' ' > cleanlist.txt
         # this adds a prefix and a suffix.
         awk '$0="search.canadiana.ca/view/"$0' cleanlist.txt| awk '{print $0 "/1?r=0&s=1&fmt=json&api_text=1"}' > urlstograb.txt
         # then if we want we can take those URLs and output them all to a big text file for analysis.
         wget -i urlstograb.txt -O output.txt

Did $ chmod 755 canadiana.sh

Did $ ./canadiana.sh

The program ran, but I got no data

I checked the URL I was searching on

pages=$(curl 'http://search.canadiana.ca/search?q="shawville"*&field=&so=score&df=1883&dt=1976&fmt=json' | jq '.pages')

The * after "shawville" is a typo, so I fixed that.

results.txt has this:

{  "key": "oocihm.9_08052_16_10" }

cleanlist.txt has this:

oocihm.9_08052_16_10

#### Getting a lot of results back

I ran this, and then saw I was getting a lot of files so I cancelled the run.  I did not want to pollute DHBox (my output.txt was 340 mb) So I refined my search using Shawville as the city.  I kept getting huge downloads, regardless of the time period.  I also used a couple other towns. "Why am I getting all of this for New Germany, Nova Scotia?" I thought. 

I executed
curl 'http://search.canadiana.ca/search/'${i}'?q=montenegr*&field=&so=score&df=1914&dt=1918&fmt=json' | jq '.docs[] | {key}' >> results.txt

then did nano results.txt - it showed I had 60 lines.

Ran the curl command again, results.txt had 90 lines.

Again, 120 lines.

It was appending my various searches (and mixing results)

I got better results when I deleted the working files before running $ ./ canadiana.txt    I added this to the top of the program:  # clean up from the previous session
          rm results.txt
          rm cleanlist.txt
          rm urlstograb.txt

This is uploaded to 

https://github.com/jeffblackadar/module2e4

## Exercise 5

Repository of files for this exercise:

https://github.com/jeffblackadar/m2e5

Configured a Twitter App (very cool)

:~$ pip install twarc

Thanks to @sarahmcole for her tip not to use sudo to execute this command, it produced and error message.

Performed:

$ twarc search hist3814 > search.json

#### Fail: search.json has 0 results

Searching Twitter for HIST3814 showed one result from less than 2 weeks ago on July 11, I was expecting to see that at least.

Performed:

$ twarc search canada150 > search.json

This worked better search.json was 80+mb

I got disconnected from Carleton's VPN in the middle of this, so I'm not sure if the task completed.

I will re-run it after I do other interactive work

Performed to install json2csv:

$ sudo npm install json2csv --save -g

Performed:

$ json2csv -i search.json -o out.csv

Got an error, I was expecting this per a post by @sarahmcole in our course Slack channel - Thanks for posting this!

          module.js:433
              throw err;
              ^
          SyntaxError: /home/jeffblackadar/search.json: Unexpected token {

Opened @sarahmcole's program

          # code inspired by https://stackoverflow.com/questions/4746190/find-and-replace-within-a-text-file-using-python
          # Thanks to @sarahmcole who wrote this
          n = 1
          #f1=open("input.json", "r")
          #for my purposes changed the above line to:
          f1=open("search.json", "r")
          f2=open("output.json", "w")
          for line in f1:
            line = line.replace('"tweet"', '"tweet' + str(n) + '"')
            n = n+1
            f2.write(line)
          f1.close()
          f2.close()

I also referenced her excellent fail log for this

https://github.com/sarahmcole/hist3814o_module2/blob/master/faillog_module2.md

After an error telling my input.json did not exist, I edited line:           f1=open("input.json", "r") given I had not created that file.

Performed:

python replace-json.py

Program worked

Performed

$ json2csv -i output.json -o out.csv

I still got an error.  My output.json and search.json are the same file size.  I expected output.json to be slightly larger.  At 168 mb they are hard files to work with.  I will run a new search with a (I think) smaller result so I can see what is going on.

Performed

$ twarc search lunenburg > search.json

ran this modifued program:

     # code inspired by https://stackoverflow.com/questions/4746190/find-and-replace-within-a-text-file-using-python
     # Thanks to @sarahmcole who wrote this
     n = 1
     f1=open("search.json", "r")
     f2=open("output.json", "w")
     #
     f2.write("{")
     for line in f1:
     #  line = line.replace('"tweet"', '"tweet' + str(n) + '"')
             addComma=","
             if n==1:
                     addComma=""
             line = addComma+'"tweet' + str(n) + '":'+line
             n = n+1
             f2.write(line)
     f2.write("}")
     f1.close()
     f2.close()


:~$ json2csv -i output.json -f name,location,text

I checked the json formatting at https://jsonformatter.curiousconcept.com/ , it was "correct"

It "runs" but I don't get much

     "name","location","text"
     ,,

I then ran this program to make a simple (or so I thought) json array with [{},{},{}]

     # code inspired by https://stackoverflow.com/questions/4746190/find-and-replace-within-a-text-file-using-python
     # Thanks to @sarahmcole who wrote this
     n = 1
     f1=open("search.json", "r")
     f2=open("output2.json", "w")
     #
     f2.write("[\n")
     for line in f1:
             addComma=","
             if n==1:
                     addComma=""
             n = n+1
             f2.write(line)
     f2.write("\n]")
     f1.close()
     f2.close()

I checked the json, it was "correct",but when I run json2csv I get an error

     :~$ json2csv -i output2.json -f name,location,text
     module.js:433
         throw err;
         ^
     SyntaxError: /home/jeffblackadar/output2.json: Unexpected token {
     
Basically, for me the json can be modified to work with json2csv, but it produces a .csv with only 2 rows (see .csv below)

https://github.com/jeffblackadar/m2e5/blob/master/out.csv

I can produce a json file that looks correctly formatted, but json2csv has errors when parsing it.
     
### A bit more with Twitter json

The json is complicated for each tweet.  I took one line and put it into 

http://jsonviewer.stack.hu/

and there I could see how it was organized

There are a number of json to csv conversion websites and tools on the internet.  Several of them look pretty shady.

Reading through for solutions to my problems above I ended up looking at jq

I could use jq to make a .csv (although using text from tweets I would need to change any commas in there)

each json entity starts with a . and some entities (like name) are subentities: .user.name

jq '.user.name+"\"~\""+.user.location+"\"~\""+.lang+"\"~\""+.text+"\""' search.json > outjq.csv

I was able to produce a hacked together file delimited with ~ characters

https://github.com/jeffblackadar/m2e5/blob/master/outjq.csv







## Exercise 6

# Module 2
## Exercise 1

## Exercise 2
Learning to use wget.

I thought about using my home machine to hold the files for the Shawville Equity so that I don't clog DHBox and also so that I don't have to move things in the middle of the project.  For now I will use DHBox so that I do what the rest of the class is doing.

### wget

Example:

wget -r --no-parent -w 5 --limit-rate=20k http://activehistory.ca/papers/

I tried --random-wait - I got an error.

:~/Spoon-Knife/wget-practice$ wget -r --no-parent -w 5 --random-wait --limit-rate=20k http://activehistory.ca/papers/

idn_encode failed (1): ‘String preparation failed’

idn_encode failed (1): ‘String preparation failed’

--2017-07-18 01:44:15--  http://%C3%A2%C2%80%C2%93%C3%A2--random-wait/

Resolving â\302\200\302\223â--random-wait (â\302\200\302\223â--random-wait)... failed: Name or service not known.

wget: unable to resolve host address ‘â\302\200\302\223â--random-wait’

#### Wget of the Shawville Equity

Started with 1 year

wget -r --no-parent -w 5 --limit-rate=20k http://collections.banq.qc.ca:8008/jrn03/equity/src/1883/

** fail ** I saw I was downloading pdfs.  Not needed.

I should be using a command like this

wget http://collections.banq.qc.ca:8008/jrn03/equity/src/1883/ -A **.txt** -r --no-parent -nd –w 2 --limit-rate=20k

### Examining downloaded files.

#### I encountered directory collections.banq.qc.ca:8008

it's too long to type all the time, tried to rename it

:~/final-project/equity/src$ rename collections.banq.qc.ca:8008 collections

syntax error at (eval 1) line 1, near "ca:"

:~/final-project/equity/src$ rename "collections.banq.qc.ca:8008" collections

syntax error at (eval 1) line 1, near "ca:"

This failed, but I was able to rename it in the file 

#### It appears I downloaded everything except .txt

:~/final-project/equity/src$ ls

collections

:~/final-project/equity/src$ cd collections

:~/final-project/equity/src/collections$ dir

jrn03

:~/final-project/equity/src/collections$ cd jrn03

:~/final-project/equity/src/collections/jrn03$ dir

equity

:~/final-project/equity/src/collections/jrn03$ cd equity

:~/final-project/equity/src/collections/jrn03/equity$ dir

src

:~/final-project/equity/src/collections/jrn03/equity$ cd src

:~/final-project/equity/src/collections/jrn03/equity/src$ dir

1883

:~/final-project/equity/src/collections/jrn03/equity/src$ cd 1883

:~/final-project/equity/src/collections/jrn03/equity/src/1883$ dir

06  07  08  09  10  11  12  index.html

:~/final-project/equity/src/collections/jrn03/equity/src/1883$ cd 06

:~/final-project/equity/src/collections/jrn03/equity/src/1883/06$ dir

07  14  21  28  index.html

:~/final-project/equity/src/collections/jrn03/equity/src/1883/06$ cd 07

:~/final-project/equity/src/collections/jrn03/equity/src/1883/06/07$ dir

83471_1883-06-07.pdf  index.html

Performed

wget http://collections.banq.qc.ca:8008/jrn03/equity/src/1883/ -A .txt -r --no-parent -nd –w 2 --limit-rate=20k

Results are now what I expect.

:~/final-project/equity$ dir

83471_1883-06-07.txt  83471_1883-07-05.txt  83471_1883-08-02.txt (....etc.)

#### Quality of OCR examples:

George Elliot's nmrsliang is Ireyond my to her children's sympathy.

Paul pro]>oiKil to <j lit up these lnmleh 1 into fragmenta,

## Close Reading with TEI

What makes you believe this site is a trustworthy provider of historical texts?

Registered charity with a number.  Is a .org web site.  Has a street address.

What makes you believe this site is NOT a trustworthy provider of historical texts?

The look of the site is somewhat dated.  There are not many names of actual people who work on the website.  It's not part of a larger institution

Opened 'blanktemplate.txt' in Netbeans (for the notepad function)

renamed file to page19.xml

Transcribed a half a page of text manually, separating each paragraph with </p> (to close the paragraph) and <p> to open a new para.

I made an .jpeg of each column of text, save that file, and uploaded it to freeocr.com.

I ran one image through freeocr.com and got some text and some unreadble characters. However, it was faster to use some of the good text than to type it.

I converted the second image to just a black and white image and this reduced some of the background smudging that stopped good character recognition.  I OCR'd that and the results, albiet for a different column of text, seemed better.

### Failures

I opened up page19.xml in Chrome, I didn't see anything per a security warning that I can't open files from my PC that reference other files (see note about .xsl below) in Chrome.  This was in the instructions, I should have read more carefully.

I opened page19.xml in Microsoft Edge and it looked like a unformed text.

I had to correct two things

In the reference tag for place names, there was an ampersand &amp; in the URL.  I had to replace the &amp; with &amp;amp;

so that ...Virginia&amp;params...  became   ...Virginia**&amp;amp;**params...

I also have to look at line 2 of the xml file that references an .xsl file (the style)  I changed the name of the .xsl to page19style.xsl and then matched that file name in the declaration of where the style sheet is in the xml file (see line below)

<?xml-stylesheet type="text/xsl" href="page19style.xsl"?>

After that it I could see style rendering and the tags working.

I will return to do more when there is time.

## Exercise 4

Performed:

http://search.canadiana.ca/search?q=%22New+Germany%22&field=&so=score&df=1800&dt=1900**&fmt=json**

Got back JSON data!  - This is wow.

         "creator" : [
            "DesBrisay, Mather B. (Mather Byles), 1828-1896"
         ],
         "note" : [
            "3 microfiches (141 fr.).",
            "\"Follow me back through a hoary century.\"",
            "Includes index.",
            "264 p."
         ],
         "title" : [
            "History of the County of Lunenburg",
            "CIHM/ICMH microfiche series ; no. 06874"
         ],


As an aside, I like how this code looks above.  Not sure how to make it look like that when I want.

#### Installed jq

         :~$ sudo apt-get install jq -y
         :~$ mkdir m2e4
         :~$ cd m2e4
         :~/m2e4$ pwd
         /home/jeffblackadar/m2e4
         :~/m2e4$ touch canadiana.sh
         :~/m2e4$ nano canadiana.sh

Further aside, if I add 10 empty spaces, I get the code formatting above.  Looks better to me.

### edit Ian Milligan's program

         #! /bin/bash
         pages=$(curl 'http://search.canadiana.ca/search?q="shawville"*&field=&so=score&df=1883&dt=1976&fmt=json' | jq '.pages')
         # this goes into the results and reads the value of 'pages' in each one of them.
         # it then tells us how many pages we're going to have.
         echo "Pages:"$pages
         # this says 'for each one of these pages, download the 'key' value on each page'
         for i in $(seq 1 $pages)
         do
                 curl 'http://search.canadiana.ca/search/'${i}'?q=montenegr*&field=&so=score&df=1883&dt=1976&fmt=json' | jq '.docs[] | {key}' >> results.txt
         done
         # the next two lines take the results.txt file that is quite messy and clean it up.
         # I'll try to explain what they all mean. 
         # Basically, tr deletes a given character - so we delete quotation marks "\"" 
         # (the slash tells the computer not to treat the quotation mark as a computer code, but as a quotation mark itself), 
         # to erase spaces, and to find the phrase "key:" and delete it too.
         sed -e 's/\"key\": \"//g' results.txt | tr -d "\"" | tr -d "{" | tr -d "}" | tr -s " " | sed '/^\s*$/d' | tr -d ' ' > cleanlist.txt
         # this adds a prefix and a suffix.
         awk '$0="search.canadiana.ca/view/"$0' cleanlist.txt| awk '{print $0 "/1?r=0&s=1&fmt=json&api_text=1"}' > urlstograb.txt
         # then if we want we can take those URLs and output them all to a big text file for analysis.
         wget -i urlstograb.txt -O output.txt

Did $ chmod 755 canadiana.sh

Did $ ./canadiana.sh

The program ran, but I got no data

I checked the URL I was searching on

pages=$(curl 'http://search.canadiana.ca/search?q="shawville"*&field=&so=score&df=1883&dt=1976&fmt=json' | jq '.pages')

The * after "shawville" is a typo, so I fixed that.

results.txt has this:

{  "key": "oocihm.9_08052_16_10" }

cleanlist.txt has this:

oocihm.9_08052_16_10

#### Getting a lot of results back

I ran this, and then saw I was getting a lot of files so I cancelled the run.  I did not want to pollute DHBox (my output.txt was 340 mb) So I refined my search using Shawville as the city.  I kept getting huge downloads, regardless of the time period.  I also used a couple other towns. "Why am I getting all of this for New Germany, Nova Scotia?" I thought. 

I executed
curl 'http://search.canadiana.ca/search/'${i}'?q=montenegr*&field=&so=score&df=1914&dt=1918&fmt=json' | jq '.docs[] | {key}' >> results.txt

then did nano results.txt - it showed I had 60 lines.

Ran the curl command again, results.txt had 90 lines.

Again, 120 lines.

It was appending my various searches (and mixing results)

I got better results when I deleted the working files before running $ ./ canadiana.txt    I added this to the top of the program:  # clean up from the previous session
          rm results.txt
          rm cleanlist.txt
          rm urlstograb.txt

This is uploaded to 

https://github.com/jeffblackadar/module2e4

## Exercise 5

Repository of files for this exercise:

https://github.com/jeffblackadar/m2e5

Configured a Twitter App (very cool)

:~$ pip install twarc

Thanks to @sarahmcole for her tip not to use sudo to execute this command, it produced and error message.

Performed:

$ twarc search hist3814 > search.json

#### Fail: search.json has 0 results

Searching Twitter for HIST3814 showed one result from less than 2 weeks ago on July 11, I was expecting to see that at least.

** Follow up **

Per Dr. Graham's coaching, I used hist3814o 

$ twarc search hist3814o > search.json

This returned a fair number of results, such as:

       "Shawn Graham\"SERA, PA406, Carleton U\"en\"~\"@edsu I have students using twarc they all keep telling me they're getting json errors, see eg https://t.co/Nvt61SRIjA\""
       "Shawn Graham\"SERA, PA406, Carleton U\"en\"~\"This stuff makes my day https://t.co/PqdcomtNwX 'I’ve realized my own power as a digital historian'\""

-- End of follow up --

Performed:

$ twarc search canada150 > search.json

This worked better search.json was 80+mb

I got disconnected from Carleton's VPN in the middle of this, so I'm not sure if the task completed.

I will re-run it after I do other interactive work

Performed to install json2csv:

$ sudo npm install json2csv --save -g

Performed:

$ json2csv -i search.json -o out.csv

Got an error, I was expecting this per a post by @sarahmcole in our course Slack channel - Thanks for posting this!

          module.js:433
              throw err;
              ^
          SyntaxError: /home/jeffblackadar/search.json: Unexpected token {

Opened @sarahmcole's program

          # code inspired by https://stackoverflow.com/questions/4746190/find-and-replace-within-a-text-file-using-python
          # Thanks to @sarahmcole who wrote this
          n = 1
          #f1=open("input.json", "r")
          #for my purposes changed the above line to:
          f1=open("search.json", "r")
          f2=open("output.json", "w")
          for line in f1:
            line = line.replace('"tweet"', '"tweet' + str(n) + '"')
            n = n+1
            f2.write(line)
          f1.close()
          f2.close()

I also referenced her excellent fail log for this

https://github.com/sarahmcole/hist3814o_module2/blob/master/faillog_module2.md

After an error telling my input.json did not exist, I edited line:           f1=open("input.json", "r") given I had not created that file.

Performed:

python replace-json.py

Program worked

Performed

$ json2csv -i output.json -o out.csv

I still got an error.  My output.json and search.json are the same file size.  I expected output.json to be slightly larger.  At 168 mb they are hard files to work with.  I will run a new search with a (I think) smaller result so I can see what is going on.

Performed

$ twarc search lunenburg > search.json

ran this modifued program:

     # code inspired by https://stackoverflow.com/questions/4746190/find-and-replace-within-a-text-file-using-python
     # Thanks to @sarahmcole who wrote this
     n = 1
     f1=open("search.json", "r")
     f2=open("output.json", "w")
     #
     f2.write("{")
     for line in f1:
     #  line = line.replace('"tweet"', '"tweet' + str(n) + '"')
             addComma=","
             if n==1:
                     addComma=""
             line = addComma+'"tweet' + str(n) + '":'+line
             n = n+1
             f2.write(line)
     f2.write("}")
     f1.close()
     f2.close()


:~$ json2csv -i output.json -f name,location,text

I checked the json formatting at https://jsonformatter.curiousconcept.com/ , it was "correct"

It "runs" but I don't get much

     "name","location","text"
     ,,

I then ran this program to make a simple (or so I thought) json array with [{},{},{}]

     # code inspired by https://stackoverflow.com/questions/4746190/find-and-replace-within-a-text-file-using-python
     # Thanks to @sarahmcole who wrote this
     n = 1
     f1=open("search.json", "r")
     f2=open("output2.json", "w")
     #
     f2.write("[\n")
     for line in f1:
             addComma=","
             if n==1:
                     addComma=""
             n = n+1
             f2.write(line)
     f2.write("\n]")
     f1.close()
     f2.close()

I checked the json, it was "correct",but when I run json2csv I get an error

     :~$ json2csv -i output2.json -f name,location,text
     module.js:433
         throw err;
         ^
     SyntaxError: /home/jeffblackadar/output2.json: Unexpected token {
     
Basically, for me the json can be modified to work with json2csv, but it produces a .csv with only 2 rows (see .csv below)

https://github.com/jeffblackadar/m2e5/blob/master/out.csv

I can produce a json file that looks correctly formatted, but json2csv has errors when parsing it.
     
### A bit more with Twitter json

The json is complicated for each tweet.  I took one line and put it into 

http://jsonviewer.stack.hu/

and there I could see how it was organized

There are a number of json to csv conversion websites and tools on the internet.  Several of them look pretty shady.

Reading through for solutions to my problems above I ended up looking at jq

I could use jq to make a .csv (although using text from tweets I would need to change any commas in there)

each json entity starts with a . and some entities (like name) are subentities: .user.name

$ jq '.user.name+"\""+.user.location+"\""+.lang+"\"~\""+.text+"\""' search.json

** Please note that the quotes inside the quotes """ need to be typed in with a backslash \ in Linux or else you will get "error: Invalid character"  **

I was able to produce a hacked together file delimited with ~ characters

https://github.com/jeffblackadar/m2e5/blob/master/outjq.csv




## Exercise 6

Set up a directory and started to install software

      :~$ mkdir ocr-test
      :~$ cd ocr-test
      :~/ocr-test$ sudo apt-get install tesseract-ocr

      $ sudo apt-get install imagemagick
      
      $ sudo apt-get install pdftk
            
      wget http://collections.banq.qc.ca:8008/jrn03/equity/src/1957/07/04/83471_1957-07-04.pdf --no-parent -nd –w 2 --limit-rate=20k
     
(My 20k limit made this download take about 15 minutes)
      
$ pdftk 83471_1957-07-04.pdf burst

I ran $ convert -density 300 pg_0001.pdf -depth 8 -strip -background white -alpha off file.tiff 

and ended up with a 600 mb file.

$ tesseract file.tiff output.txt

Produced an output.txt with no useful text.  Example:  "m Mmmum gum .uumuuwuum "

I ran $ convert -density 100 pg_0001.pdf -depth 8 -strip -background white -alpha off file.tiff  and got a 70 mb file that I downloaded to examine, it looked fine, if quite large.

Still, $ tesseract file.tiff output.txt produced unusable text.

I worked with the .tiff localled, but I was unable to upload it to DHBox.  It would stop after uploading about 250k of data.  I rebotted my machine and this remained the case.

I uploaded a small image of text I used for OCR for exercise 3.  Because it was small, it uploaded.  I ran tesseract and it worked better:

      been grndnelly rednend by rhe opereeihn er lnwr origin-
      ally derigned rer their comfort, med prmeclilyn. I know
      also ihhi many‘ﬂ‘avﬁ: [lass their live: in wmpemive
      we '“P‘l'svm ,"w “I209 ’ 9f ‘ MM
      lhn lhe we: weighed of our penper; 3h: envy the
      
      .llnimem of «he ham Negro. This is hm. however.
      
      nemes er the two classes ihm 

So tesseract works, but I need to do more work with the image from the Shawville Equity.

## Other Stuff

I accidentally closed the Chrome tab without saving a bunch of work in the fail log.  I was able to CTRL+SHIFT+T and get it back.  



