# Module 2
## Exercise 1

## Exercise 2
Learning to use wget.

I thought about using my home machine to hold the files for the Shawville Equity so that I don't clog DHBox and also so that I don't have to move things in the middle of the project.  For now I will use DHBox so that I do what the rest of the class is doing.

### wget

Example:

wget -r --no-parent -w 5 --limit-rate=20k http://activehistory.ca/papers/

I tried --random-wait - I got an error.

:~/Spoon-Knife/wget-practice$ wget -r --no-parent -w 5 --random-wait --limit-rate=20k http://activehistory.ca/papers/

idn_encode failed (1): ‘String preparation failed’

idn_encode failed (1): ‘String preparation failed’

--2017-07-18 01:44:15--  http://%C3%A2%C2%80%C2%93%C3%A2--random-wait/

Resolving â\302\200\302\223â--random-wait (â\302\200\302\223â--random-wait)... failed: Name or service not known.

wget: unable to resolve host address ‘â\302\200\302\223â--random-wait’

#### Wget of the Shawville Equity

Started with 1 year

wget -r --no-parent -w 5 --limit-rate=20k http://collections.banq.qc.ca:8008/jrn03/equity/src/1883/

** fail ** I saw I was downloading pdfs.  Not needed.

I should be using a command like this

wget http://collections.banq.qc.ca:8008/jrn03/equity/src/1883/ -A **.txt** -r --no-parent -nd –w 2 --limit-rate=20k

### Examining downloaded files.

#### I encountered directory collections.banq.qc.ca:8008

it's too long to type all the time, tried to rename it

:~/final-project/equity/src$ rename collections.banq.qc.ca:8008 collections

syntax error at (eval 1) line 1, near "ca:"

:~/final-project/equity/src$ rename "collections.banq.qc.ca:8008" collections

syntax error at (eval 1) line 1, near "ca:"

This failed, but I was able to rename it in the file 

#### It appears I downloaded everything except .txt

:~/final-project/equity/src$ ls

collections

:~/final-project/equity/src$ cd collections

:~/final-project/equity/src/collections$ dir

jrn03

:~/final-project/equity/src/collections$ cd jrn03

:~/final-project/equity/src/collections/jrn03$ dir

equity

:~/final-project/equity/src/collections/jrn03$ cd equity

:~/final-project/equity/src/collections/jrn03/equity$ dir

src

:~/final-project/equity/src/collections/jrn03/equity$ cd src

:~/final-project/equity/src/collections/jrn03/equity/src$ dir

1883

:~/final-project/equity/src/collections/jrn03/equity/src$ cd 1883

:~/final-project/equity/src/collections/jrn03/equity/src/1883$ dir

06  07  08  09  10  11  12  index.html

:~/final-project/equity/src/collections/jrn03/equity/src/1883$ cd 06

:~/final-project/equity/src/collections/jrn03/equity/src/1883/06$ dir

07  14  21  28  index.html

:~/final-project/equity/src/collections/jrn03/equity/src/1883/06$ cd 07

:~/final-project/equity/src/collections/jrn03/equity/src/1883/06/07$ dir

83471_1883-06-07.pdf  index.html

Performed

wget http://collections.banq.qc.ca:8008/jrn03/equity/src/1883/ -A .txt -r --no-parent -nd –w 2 --limit-rate=20k

Results are now what I expect.

:~/final-project/equity$ dir

83471_1883-06-07.txt  83471_1883-07-05.txt  83471_1883-08-02.txt (....etc.)

#### Quality of OCR examples:

George Elliot's nmrsliang is Ireyond my to her children's sympathy.

Paul pro]>oiKil to <j lit up these lnmleh 1 into fragmenta,

## Close Reading with TEI

What makes you believe this site is a trustworthy provider of historical texts?

Registered charity with a number.  Is a .org web site.  Has a street address.

What makes you believe this site is NOT a trustworthy provider of historical texts?

The look of the site is somewhat dated.  There are not many names of actual people who work on the website.  It's not part of a larger institution

Opened 'blanktemplate.txt' in Netbeans (for the notepad function)

renamed file to page19.xml

Transcribed a half a page of text manually, separating each paragraph with </p> (to close the paragraph) and <p> to open a new para.

I made an .jpeg of each column of text, save that file, and uploaded it to freeocr.com.

I ran one image through freeocr.com and got some text and some unreadble characters. However, it was faster to use some of the good text than to type it.

I converted the second image to just a black and white image and this reduced some of the background smudging that stopped good character recognition.  I OCR'd that and the results, albiet for a different column of text, seemed better.

### Failures

I opened up page19.xml in Chrome, I didn't see anything per a security warning that I can't open files from my PC that reference other files (see note about .xsl below) in Chrome.  This was in the instructions, I should have read more carefully.

I opened page19.xml in Microsoft Edge and it looked like a unformed text.

I had to correct two things

In the reference tag for place names, there was an ampersand &amp; in the URL.  I had to replace the &amp; with &amp;amp;

so that ...Virginia&amp;params...  became   ...Virginia**&amp;amp;**params...

I also have to look at line 2 of the xml file that references an .xsl file (the style)  I changed the name of the .xsl to page19style.xsl and then matched that file name in the declaration of where the style sheet is in the xml file (see line below)

<?xml-stylesheet type="text/xsl" href="page19style.xsl"?>

After that it I could see style rendering and the tags working.

I will return to do more when there is time.

## Exercise 4

Performed:

http://search.canadiana.ca/search?q=%22New+Germany%22&field=&so=score&df=1800&dt=1900**&fmt=json**

Got back JSON data!  - This is wow.

         "creator" : [
            "DesBrisay, Mather B. (Mather Byles), 1828-1896"
         ],
         "note" : [
            "3 microfiches (141 fr.).",
            "\"Follow me back through a hoary century.\"",
            "Includes index.",
            "264 p."
         ],
         "title" : [
            "History of the County of Lunenburg",
            "CIHM/ICMH microfiche series ; no. 06874"
         ],


As an aside, I like how this code looks above.  Not sure how to make it look like that when I want.

#### Installed jq

         :~$ sudo apt-get install jq -y
         :~$ mkdir m2e4
         :~$ cd m2e4
         :~/m2e4$ pwd
         /home/jeffblackadar/m2e4
         :~/m2e4$ touch canadiana.sh
         :~/m2e4$ nano canadiana.sh

Further aside, if I add 10 empty spaces, I get the code formatting above.  Looks better to me.

### edit Ian Milligan's program

         #! /bin/bash
         pages=$(curl 'http://search.canadiana.ca/search?q="shawville"*&field=&so=score&df=1883&dt=1976&fmt=json' | jq '.pages')
         # this goes into the results and reads the value of 'pages' in each one of them.
         # it then tells us how many pages we're going to have.
         echo "Pages:"$pages
         # this says 'for each one of these pages, download the 'key' value on each page'
         for i in $(seq 1 $pages)
         do
                 curl 'http://search.canadiana.ca/search/'${i}'?q=montenegr*&field=&so=score&df=1883&dt=1976&fmt=json' | jq '.docs[] | {key}' >> results.txt
         done
         # the next two lines take the results.txt file that is quite messy and clean it up.
         # I'll try to explain what they all mean. 
         # Basically, tr deletes a given character - so we delete quotation marks "\"" 
         # (the slash tells the computer not to treat the quotation mark as a computer code, but as a quotation mark itself), 
         # to erase spaces, and to find the phrase "key:" and delete it too.
         sed -e 's/\"key\": \"//g' results.txt | tr -d "\"" | tr -d "{" | tr -d "}" | tr -s " " | sed '/^\s*$/d' | tr -d ' ' > cleanlist.txt
         # this adds a prefix and a suffix.
         awk '$0="search.canadiana.ca/view/"$0' cleanlist.txt| awk '{print $0 "/1?r=0&s=1&fmt=json&api_text=1"}' > urlstograb.txt
         # then if we want we can take those URLs and output them all to a big text file for analysis.
         wget -i urlstograb.txt -O output.txt

Did $ chmod 755 canadiana.sh

Did $ ./canadiana.sh

The program ran, but I got no data

I checked the URL I was searching on

pages=$(curl 'http://search.canadiana.ca/search?q="shawville"*&field=&so=score&df=1883&dt=1976&fmt=json' | jq '.pages')

The * after "shawville" is a typo, so I fixed that.

results.txt has this:

{  "key": "oocihm.9_08052_16_10" }

cleanlist.txt has this:

oocihm.9_08052_16_10

#### Getting a lot of results back

I ran this, and then saw I was getting a lot of files so I cancelled the run.  I did not want to pollute DHBox (my output.txt was 340 mb) So I refined my search using Shawville as the city.  I kept getting huge downloads, regardless of the time period.  I also used a couple other towns. "Why am I getting all of this for New Germany, Nova Scotia?" I thought. 

I executed
curl 'http://search.canadiana.ca/search/'${i}'?q=montenegr*&field=&so=score&df=1914&dt=1918&fmt=json' | jq '.docs[] | {key}' >> results.txt

then did nano results.txt - it showed I had 60 lines.

Ran the curl command again, results.txt had 90 lines.

Again, 120 lines.

It was appending my various searches (and mixing results)

I got better results when I deleted the working files before running $ ./ canadiana.txt    I added this to the top of the program:  # clean up from the previous session
          rm results.txt
          rm cleanlist.txt
          rm urlstograb.txt

This is uploaded to 

https://github.com/jeffblackadar/module2e4

## Exercise 5

Configured a Twitter App (very cool)

:~$ pip install twarc

Thank to @sarahmcole for her tip not to use sudo to execute this command, it produced and error message.

Performed:

$ twarc search hist3814 > search.json

#### Fail: search.json has 0 results

Searching Twitter for HIST3814 showed one result from less than 2 weeks ago on July 11, I was expecting to see that at least.

Performed:

$ twarc search canada150 > search.json

This worked better search.json was 80+mb

I got disconnected from Carleton's VPN in the middle of this, so I'm not sure if the task completed.

I will re-run it after I do other interactive work

Performed to install json2csv:

$ sudo npm install json2csv --save -g

Performed:

$ json2csv -i search.json -o out.csv

Got an error, I was expecting this per a post by @sarahmcole in our course Slack channel - Thanks for posting this!

          module.js:433
              throw err;
              ^
          SyntaxError: /home/jeffblackadar/search.json: Unexpected token {

Opened @sarahmcole's program

          # code inspired by https://stackoverflow.com/questions/4746190/find-and-replace-within-a-text-file-using-python
          # Thanks to @sarahmcole who wrote this
          n = 1
          #f1=open("input.json", "r")
          #for my purposes changed the above line to:
          f1=open("search.json", "r")
          f2=open("output.json", "w")
          for line in f1:
            line = line.replace('"tweet"', '"tweet' + str(n) + '"')
            n = n+1
            f2.write(line)
          f1.close()
          f2.close()

I also referenced her excellent fail log for this

https://github.com/sarahmcole/hist3814o_module2/blob/master/faillog_module2.md

After an error telling my input.json did not exist, I edited line:           f1=open("input.json", "r") given I had not created that file.

Performed:

python replace-json.py

Program worked

Performed

$ json2csv -i output.json -o out.csv

### Fail 

I still got an error.

my output.json and search.json are the same file size.  I expected output.json to be slightly larger.  At 168 mb they are hard files to work with.  I will run a new search with a (I think) smaller result so I can see what is going on.

Performed

$ twarc search lunenburg > search.json

ran this modifued program:

     # code inspired by https://stackoverflow.com/questions/4746190/find-and-replace-within-a-text-file-using-python
     # Thanks to @sarahmcole who wrote this
     n = 1
     f1=open("search.json", "r")
     f2=open("output.json", "w")
     #
     f2.write("{")
     for line in f1:
     #  line = line.replace('"tweet"', '"tweet' + str(n) + '"')
             addComma=","
             if n==1:
                     addComma=""
             line = addComma+'"tweet' + str(n) + '":'+line
             n = n+1
             f2.write(line)
     f2.write("}")
     f1.close()
     f2.close()


:~$ json2csv -i output.json -f name,location,text

I checked the json formatting at https://jsonformatter.curiousconcept.com/ , it was "correct"

It "runs" but I don't get much

     "name","location","text"
     ,,

I then ran this program to make a simple (or so I thought) json array with [{},{},{}]

     # code inspired by https://stackoverflow.com/questions/4746190/find-and-replace-within-a-text-file-using-python
     # Thanks to @sarahmcole who wrote this
     n = 1
     f1=open("search.json", "r")
     f2=open("output2.json", "w")
     #
     f2.write("[\n")
     for line in f1:
             addComma=","
             if n==1:
                     addComma=""
             n = n+1
             f2.write(line)
     f2.write("\n]")
     f1.close()
     f2.close()

I checked the json, it was "correct",but when I run json2csv I get an error

     :~$ json2csv -i output2.json -f name,location,text
     module.js:433
         throw err;
         ^
     SyntaxError: /home/jeffblackadar/output2.json: Unexpected token {

The json is complicated for each tweet.  I took one line and put it into 

http://jsonviewer.stack.hu/

and there I could see how it was organized

There are a number of json to csv conversion websites and tools on the internet.  Several of them look pretty shady.

Reading through for solutions to my problems above I ended up looking at jq

I could use jq to make a .csv (although using text from tweets I would need to change any commas in there)

each json entity starts with a . and some entities (like name) are subentities: .user.name

$ jq '.text+","+.lang+","+.user.name+","+.user.location' search.json







## Exercise 6




## Other Stuff

I accidentally closed the Chrome tab without saving a bunch of work in the fail log.  I was able to CTRL+SHIFT+T and get it back.  

Quick recipe to make a repository

On github make a repository e.g. m2e5

