# Fail log for final project

## Content Management

### Installed Mahara on jeffblackdar.ca

latest version with automatic updates installation (per best practice)

Rationale: It is a platform flexible enough to present information and one I have used before.

#### Equity raw files

Could not upload corpus of text files of Equity to Voyant, it took too long. The file was too large for Github.  I created a folder on jeffblackadar.ca in public_html called hist3814o_final to hold this.  I plan to point Voyant to this file.

# Finding aid for the Equity

Searching through the Equity for content relating to various annual events meant clicking and a little guesswork to determine when, for example, the first edition was published after an Agricultural Fair that was held the 3rd weekend each September.  My eyes and wrist were sore.  I thought a finding aid with a list of editions and any significant events might be easier to handle.

So I downloaded just the titles of the Equity using the wget spider command

wget --spider --force-html -r -w1 -np http://collections.banq.qc.ca:8008/jrn03/equity/

Performed

ls -R 

I piped this into a file and used grep to get just a list of partial URLs, example:

http://collections.banq.qc.ca:8008/jrn03/equity/src/1990/05/02/


## R Studio work on the finding aid

Opened R Studio.

Opened a project from an existing repository on Github:

https://github.com/jeffblackadar/hist3814o-final

This looks like a promising reference for what I need to do:

https://en.wikibooks.org/wiki/R_Programming/Text_Processing

Did a hello world program to read a file and print each line based on an example from Stackoverflow  https://stackoverflow.com/questions/4106764/what-is-a-good-way-to-read-line-by-line-in-r

       #file is in the same directory 
       #It has lines with 2 patterns and lengths:
       #[1] "http://collections.banq.qc.ca:8008/jrn03/equity/src/2010/08/25/"
       #[1] "http://collections.banq.qc.ca:8008/jrn03/equity/src/2010/08/25/01/"
       inputFile <- "equityurls.txt"
       #open connecton to read file
       con  <- file(inputFile, open = "r")
       while (length(oneLine <- readLines(con, n = 1, warn = FALSE)) > 0) {
           print (oneLine)
       } 
       close(con)

Progress, here is a sample from the file.  This took a while to figure our string parsing and concatenation in R. The cooking equivalent of being able to fry up some onions.

base url,yearmonthday
http://collections.banq.qc.ca:8008/jrn03/equity/src/1883/06/07/,2010,08,25

     #file is in the same directory 
     #It has lines with 2 patterns and lengths:
     #[1] "http://collections.banq.qc.ca:8008/jrn03/equity/src/2010/08/25/"
     #[1] "http://collections.banq.qc.ca:8008/jrn03/equity/src/2010/08/25/01/"

     #openconnecton to read file
     inputFile <- "equityurls.txt"
     inputCon  <- file(inputFile, open = "r")

     #open connecton to wite output file
     outputFileCsv <- "equityeditions.csv"
     outputCsvCon<-file(outputFileCsv, open = "w")
     
     #print column headers at top of csv
     writeLines(paste('base url',',','year','month','day',sep=""), outputCsvCon)
     
     while (length(urlLine <- readLines(inputCon, n = 1, warn = FALSE)) > 0) {
         urlLineElements = strsplit(stringl, "/")
         outputLine<-paste(urlLine,',',urlLineElements[[1]][7],',',urlLineElements[[1]][8],',',urlLineElements[[1]][9],sep="")
         print (outputLine)
         writeLines(outputLine, outputCsvCon)
     } 
     close(inputCon)
     close(outputCsvCon)


This program is now generating a list of dates with links

     #file is in the same directory 
     #It has lines with 2 patterns and lengths:
     #[1] "http://collections.banq.qc.ca:8008/jrn03/equity/src/2010/08/25/"
     #[1] "http://collections.banq.qc.ca:8008/jrn03/equity/src/2010/08/25/01/"

     #openconnecton to read file
     inputFile <- "equityurls.txt"
     inputCon  <- file(inputFile, open = "r")

     #open connecton to wite output file
     outputFileCsv <- "equityeditions.csv"
     outputCsvCon<-file(outputFileCsv, open = "w")

     #open connecton to wite output file
     outputFileHtml <- "equityeditions.html"
     outputFileHtmlCon<-file(outputFileHtml, open = "w")

     #print column headers at top of csv
     writeLines('base url,year,month,day,weekday', outputCsvCon)

     #set up web page at top of html
     writeLines('<html><head><title></title></head><body><h1>Editions of the Shawville Equity.</h1><table>', outputFileHtmlCon)


     while (length(urlLine <- readLines(inputCon, n = 1, warn = FALSE)) > 0) {
         urlLineElements = strsplit(urlLine, "/")
         #make sure it is a complete line
         if(length(urlLineElements[[1]])>8){
    
    
         #Note the %Y indicates a 4 year date
         dateOfEdition<-as.Date(paste(urlLineElements[[1]][7],'/',urlLineElements[[1]][8],'/',urlLineElements[[1]][9],sep=""),"%Y/%m/%d")
         #Credit to http://statmethods.net/input/dates.html
         #print day of the week
         #print(format(dateOfEdition,format="%a"))
         outputLine<-paste(urlLine,',',urlLineElements[[1]][7],',',urlLineElements[[1]][8],',',urlLineElements[[1]][9],',',format(dateOfEdition,format="%a"),sep="")
    
    
         print (outputLine)
         writeLines(outputLine, outputCsvCon)
    
         #html
         writeLines('<tr>', outputFileHtmlCon)
    
       
         #set file name, make it look like this: 83471_1920-06-10.pdf
         if (length(urlLineElements[[1]])==10){
           #editions with dates like these (/2010/08/25/01/) are supplements  
           writeLines(paste('<td><a href="',urlLine,'">',format(dateOfEdition,format='%Y/%m/%d'),' (sup.)','</a></td>',sep=""),outputFileHtmlCon)
           editionFileName<-paste('83471_',urlLineElements[[1]][7],'-',urlLineElements[[1]][8],'-',urlLineElements[[1]][9],'-',urlLineElements[[1]][10],sep='')  
    } else {
           writeLines(paste('<td><a href="',urlLine,'">',format(dateOfEdition,format='%Y/%m/%d'),'</a></td>',sep=""),outputFileHtmlCon)
           editionFileName<-paste('83471_',urlLineElements[[1]][7],'-',urlLineElements[[1]][8],'-',urlLineElements[[1]][9],sep='')        
    }

         writeLines(paste('<td><a href="',urlLine,editionFileName,'.pdf">.pdf</a></td>',sep=""),outputFileHtmlCon)
         writeLines(paste('<td><a href="',urlLine,editionFileName,'.txt">.txt</a></td>',sep=""),outputFileHtmlCon)
         writeLines('</tr>', outputFileHtmlCon)
         }
     } 

     writeLines('</table></body></html>', outputFileHtmlCon)

     close(outputFileHtmlCon)
     close(inputCon)
     close(outputCsvCon)

## Next up, generate a list of key dates for recurring events.

Dates of provincial elections

view-source:http://www.quebecpolitique.com/elections-et-referendums/circonscriptions/elections-dans-pontiac/#2008

Started working on the program, but realized I was not commiting changes.  Nor backing up my data.  I added files, committed them and pushed them to github  https://github.com/jeffblackadar/hist3814o-final

Got the dates working.... and see that in the case of 1890, the coverage of the election is more than a week later.

http://collections.banq.qc.ca:8008/jrn03/equity/src/1890/06/26/83471_1890-06-26.pdf


# Downloaded Equity

Downloaded all text of Equity 1900-1999

wget64 http://collections.banq.qc.ca:8008/jrn03/equity/src/ -A "*19*.txt" -nc -r -np -nd -w2 --limit-rate=20k

Checked quality of download: 5172 files for 1900-1999 = 51.72 files per average 52 week year.  Filenames are unique. File sizes range from  23k for edition 1949-12-22 to 253k edition 1981-12-16.   There are 212 missing editions from 1900-1999 that are 1k in size. Most files are between 100k-150k in size.

